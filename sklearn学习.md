|大类 |  小类 | 适用问题 | 实现 | 说明 |
|-------- | --------| -------- | -------- | -------- |
|1.1 [广义线性模型](http://scikit-learn.org/stable/modules/linear_model.html)| 1.1.1 普通最小二乘法  | 线性回归 | <small>[sklearn.linear_model.LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)</small> | |
| | 1.1.2 Ridge/岭回归 | 线性回归 | <small>[sklearn.linear_model.Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)</small> | 解决两类回归问题：<br>一是样本少于变量个数<br>二是变量间存在共线性 |
| | 1.1.3 Lasso  | 线性回归 | <small>[sklearn.linear_model.Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)</small> | 适合特征较少的数据 |
| | 1.1.4 Multi-task Lasso  | 线性回归 | <small>[sklearn.linear_model.MultiTaskLasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso)</small> | y值不是一元的回归问题
| | 1.1.5 Elastic Net  | 线性回归 | <small>[sklearn.linear_model.ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)</small> | 结合了Ridge和Lasso |
| | 1.1.6 Multi-task Elastic Net  | 线性回归 | <small>[sklearn.linear_model.MultiTaskElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet)</small> | y值不是一元的回归问题 |
| | 1.1.7 Least Angle Regression(LARS)  | 线性回归 | <small>[sklearn.linear_model.Lars](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html#sklearn.linear_model.Lars)</small> | 适合高维数据 |
| | 1.1.8 LARS Lasso  | 线性回归 | <small>[sklearn.linear_model.LassoLars](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars)</small> | (1)适合高维数据使用<br>(2)LARS算法实现的lasso模型 |
| | 1.1.9 Orthogonal Matching Pursuit (OMP)  | 线性回归 | <small>[sklearn.linear_model.OrthogonalMatchingPursuit](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit)</small> | 基于贪心算法实现 |
| | 1.1.10 贝叶斯回归  | 线性回归 | <small>[sklearn.linear_model.BayesianRidge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge) <br>[sklearn.linear_model.ARDRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression)</small>| 优点： (1)适用于手边数据(2)可用于在估计过程中包含正规化参数 <br>缺点：耗时 |
| | 1.1.11 Logistic regression  | 分类 | <small>[sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)</small> |
| | 1.1.12 SGD(随机梯度下降法)  | 分类<br>/线性回归 | <small>[sklearn.linear_model.SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)<br>[sklearn.linear_model.SGDRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)</small> | 适用于大规模数据 |
| | 1.1.13 Perceptron  | 分类 | <small>[sklearn.linear_model.Perceptron](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron)</small> | 适用于大规模数据 |
| | 1.1.14 Passive Aggressive Algorithms  | 分类<br>/线性回归 | <small>[sklearn.linear_model.PassiveAggressiveClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier)<br>[sklearn.linear_model.PassiveAggressiveRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveRegressor.html#sklearn.linear_model.PassiveAggressiveRegressor)</small> | 适用于大规模数据 |
| | 1.1.15 Huber Regression  | 线性回归 | <small>[sklearn.linear_model.HuberRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor)</small> | 能够处理数据中有异常值的情况
| | 1.1.16 多项式回归  | 线性回归 | <small>[sklearn.preprocessing.PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)</small> | 通过PolynomialFeatures将非线性特征转化成多项式形式，再用线性模型进行处理 |
| 1.2 [线性和二次判别分析](http://scikit-learn.org/stable/modules/lda_qda.html) | 1.2.1 LDA | 分类/降维 | <small>[sklearn.discriminant_analysis.<br>LinearDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)</small> |  |
|  | 1.2.2 QDA | 分类 | <small>[sklearn.discriminant_analysis.<br>QuadraticDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)</small> |  |
| 1.3 [核岭回归](http://scikit-learn.org/stable/modules/kernel_ridge.html) | 简称KRR | 非线性回归 | <small>[sklearn.kernel_ridge.KernelRidge](http://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge)</small> | 将核技巧应用到岭回归(1.1.2)中 |
| 1.4 [支持向量机](http://scikit-learn.org/stable/modules/svm.html) | 1.4.1 SVC,NuSVC,LinearSVC | 多类分类 | <small>[sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)<br>[sklearn.svm.NuSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC)<br>[sklearn.svm.LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)</small>| SVC可用于非线性分类，可指定核函数；<br>NuSVC与SVC唯一的不同是可控制支持向量的个数;<br>LinearSVC用于线性分类|
|  | 1.4.2 SVR,NuSVR,LinearSVR | 回归 | <small>[sklearn.svm.SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR)<br>[sklearn.svm.NuSVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVR)<br>[sklearn.svm.LinearSVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVR)</small>| 同上，将"分类"变成"回归"即可 |
|  | 1.4.3 OneClassSVM | 异常值检测 | <small>[sklearn.svm.OneClassSVM](http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM)</small>| 无监督 |
| 1.5 [随机梯度下降](http://scikit-learn.org/stable/modules/sgd.html) | 同1.1.12 |  |  |  |
| 1.6 [最近邻](http://scikit-learn.org/stable/modules/neighbors.html) | 1.6.1 Unsupervised Nearest Neighbors | 寻找K近邻 | [sklearn.neighbors.NearestNeighbors](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) | 无监督 |
| | 1.6.2 Nearest Neighbors Classification | 多类分类 | [sklearn.neighbors.KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)<br>[sklearn.neighbors.RadiusNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier) | (1)不太适用于高维数据<br>(2)两种实现只是距离度量不一样，后者更适合非均匀的采样 |
| | 1.6.3 Nearest Neighbors Regression| 回归 | [sklearn.neighbors.KNeighborsRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)<br>[sklearn.neighbors.RadiusNeighborsRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor) | 同上 |
| | 1.6.5 Nearest Centroid Classifier | 多类分类 | [sklearn.neighbors.NearestCentroid](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid) | 每个类对应一个质心，测试样本被分类到距离最近的质心所在的类别 |
| 1.7 [高斯过程(GP/GPML)](http://scikit-learn.org/stable/modules/gaussian_process.html) | 1.7.1 GPR | 回归 | [sklearn.gaussian_process.<br>GaussianProcessRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) | 与KRR一样使用了核技巧 |
|  | 1.7.3 GPC | 多类分类 | [sklearn.gaussian_process.<br>GaussianProcessClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier) |  |
| 1.8 [交叉分解](http://scikit-learn.org/stable/modules/cross_decomposition.html) | 实现算法：CCA和PLS |  |  | 用来计算两个多元数据集的线性关系，当预测数据比观测数据有更多的变量时，用PLS更好 |
| 1.9 [朴素贝叶斯](http://scikit-learn.org/stable/modules/naive_bayes.html) | 1.9.1 高斯朴素贝叶斯 | 多类分类 | [sklearn.naive_bayes.GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) | 处理特征是连续型变量的情况 |
|  | 1.9.2 多项式朴素贝叶斯 | 多类分类 | [sklearn.naive_bayes.MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) | 最常见，要求特征是离散数据 |
|  | 1.9.3 伯努利朴素贝叶斯 | 多类分类 | [sklearn.naive_bayes.BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB) | 要求特征是离散的，且为布尔类型，即true和false，或者1和0 |
| 1.10 [决策树](http://scikit-learn.org/stable/modules/tree.html) | 1.10.1 Classification | 多类分类 | [sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) |  |
|  | 1.10.2 Regression | 回归 | [sklearn.tree.DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) |  |
| 1.11 [集成方法](http://scikit-learn.org/stable/modules/ensemble.html) | 1.11.1 Bagging | 分类/回归 | [sklearn.ensemble.BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)<br>[sklearn.ensemble.BaggingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor) | 可以指定基学习器，默认为决策树 |
| 注：1和2属于集成方法中的并行化方法，3和4属于序列化方法 | 1.11.2 Forests of randomized trees | 分类/回归 | RandomForest(RF,随机森林):<br>[sklearn.ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)<br>[sklearn.ensemble.RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor)<br>ExtraTrees(RF改进):<br>[sklearn.ensemble.ExtraTreesClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier)<br>[sklearn.ensemble.ExtraTreesRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor) | 基学习器为决策树 |
| | 1.11.3 AdaBoost | 分类/回归 | [sklearn.ensemble.AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)<br>[sklearn.ensemble.AdaBoostRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) | 可以指定基学习器，默认为决策树 |
| 号外：最近特别火的两个梯度提升算法，LightGBM和XGBoost<br>(sklearn中没有集成)  | 1.11.4 Gradient Tree Boosting| 分类/回归 | GBDT:<BR>[sklearn.ensemble.GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)<br>GBRT:<BR>[sklearn.ensemble.GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor) | 基学习器为决策树 |
|  | 1.11.5 Voting Classifier | 分类 | [sklearn.ensemble.VotingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) | 须指定基学习器 |
|  |  |  |  |  |
|  |  |  |  |  |

